# -*- coding: utf-8 -*-
"""Instagram Account Classification- Fake vs Genuine

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gz7ap0AuoxBt-RPZJke-YUgMTHlQxpbL

**Step 1**: Load and preview the dataset
"""

#Importing libraries and dataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report,confusion_matrix

from google.colab import files

data = files.upload()

df= pd.read_csv('train.csv')

data = files.upload()

df_test = pd.read_csv('test.csv')

df.head()

df_test.head()



"""**Step 2**: Performing Data Cleaning & Exploratory Data Analysis(EDA) for 'train' dataset"""

#Check and handle missing values

df.info()

#Statistical Summary of the Data

df.describe()

#checking if null values exist

df.isnull().sum()

"""from above data info we can see there is no null value."""

#Number of unique values in the profile pic column

df['profile pic'].value_counts()

#Check data types

print("Data types of each column:")
print(df.dtypes)

#Number of fake and genuine accounts

df['fake'].value_counts()

#Number of accounts having an external URL

df['external URL'].value_counts()

#Number of accounts having description length over 50

(df['description length']>50).sum()

"""Here are what some features likely represent in train dataset:

*  profile pic: 1 if profile picture exists, 0 if not.

*  nums/length username: numeric ratio in username.

*  fullname words: number of words in full name.

*  nums/length fullname: numeric ratio in full name.

*  name==username: 1 if full name equals username.

*  description length: length of the bio/description.

*  external URL: 1 if profile has an external link.

*  private: 1 if account is private.

*  #posts, #followers, #follows: self-explanatory.

*  fake: Target column (0 = Genuine, 1 = Fake)
"""



"""**Step 3**: Performing Data Cleaning & Exploratory Data Analysis(EDA) for 'test' dataset"""

df_test.info()

df_test.describe()

df_test.isnull().sum()

df_test['fake'].value_counts()



"""**Step 4**: Data Visualization of 'train' dataset"""

#the number of fake eand real accounts

sns.countplot(x='fake', data=df, palette= 'PuBu')
plt.title("Number of Fake and Real Accounts")
plt.show()

#visualizing the 'private' column

sns.countplot(x='private', data=df, palette= 'Pastel2')
plt.title("Number of Private and Public Accounts")
plt.show()

#visualizing the 'profile pic' column

sns.countplot(x = 'profile pic', data= df, palette= 'PuBu')
plt.title("Number of Accounts with and without Profile Pictures")
plt.show()

#Visualizing the 'nums/length username' column

plt.figure(figsize= (10,6))

sns.distplot(df['nums/length username'], kde= True)

plt.title("Distribution of Number of Numerical Characters in Username")
plt.show()

#Correlation analysis

corr_matrix = df.corr()

plt.figure(figsize=(10,8))

#Draw the heatmap
sns.heatmap(corr_matrix, annot = True, fmt = ".2f", cmap = "coolwarm",
            square= True)

plt.title("Correlation Matrix of Instagram Account features")
plt.show()

#correlation between the each feature using histogram

df.hist(figsize=(10,10), bins=50, xlabelsize=8, ylabelsize=8, color='blue',
        edgecolor='black', linewidth=1.0, grid=False)
plt.show()



"""**Step 5**: Data Visualization of 'test' dataset"""

#the number of fake eand real accounts

sns.countplot(x='fake', data=df_test, palette= 'PuBu')
plt.title("Number of Fake and Real Accounts")
plt.show()

#visualizing the 'private' column

sns.countplot(x='private', data=df_test , palette= 'Pastel2')
plt.title("Number of Private and Public Accounts")
plt.show()

#visualizing the 'profile pic' column

sns.countplot(x = 'profile pic', data= df_test , palette= 'PuBu')
plt.title("Number of Accounts with and without Profile Pictures")
plt.show()

"""**Step 6**: Data preparation for Modeling"""

#Preparing the inputs(Dropping the fake column from both train and test dataset

x_train = df.drop('fake', axis=1)
x_test = df_test.drop('fake', axis=1)

x_train.head()

x_test.head()

#preparing outputs

y_train = df['fake']
y_test = df_test['fake']

y_train

y_test

"""Select Features and Target"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Compute correlation of each feature with target
correlation_with_target = df.corr()['fake'].drop('fake')
selected_features = correlation_with_target[correlation_with_target.abs() > 0.1].index.tolist()

# Features and target
X = df[selected_features]
y = df['fake']

# Split data into training and testing sets (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Standardize features (important for models like SVM, KNN, etc.)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest again
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)

# Get feature importances
importances = rf.feature_importances_
feature_names = X.columns

# Create a DataFrame for visualization
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(8, 5))
sns.barplot(data=feat_imp_df, x='Importance', y='Feature')
plt.title("Feature Importance - Random Forest")
plt.tight_layout()
plt.show()



y_train = np.array(y_train)
y_test = np.array(y_test)

x_train.shape, x_test.shape, y_train.shape, y_test.shape

#percentage of training data

train_data_percent = len(x_train)/(len(x_train)+len(x_test))*100
train_data_percent

test_data_percent = len(x_test)/(len(x_train)+len(x_test))*100
test_data_percent



"""**Step 7**: Build a deep learning model (using Keras & TensorFlow)

Installed Tensorflow
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential()
model.add(Dense(50, input_dim = 11, activation= 'relu'))
model.add(Dropout(0.3))
model.add(Dense(150, activation= 'relu'))
model.add(Dropout(0.3))
model.add(Dense(25, activation= 'relu'))
model.add(Dropout(0.3))
model.add(Dense(2, activation= 'softmax'))

model.summary()

model.compile(loss= 'sparse_categorical_crossentropy', optimizer= 'adam',
              metrics= ['accuracy'])

epochs_hist = model.fit(x_train, y_train, epochs= 20, batch_size= 32,
                        validation_data= (x_test, y_test))

"""**Step 8**: Model Performamce Assessment"""

print(epochs_hist.history.keys())

plt.plot(epochs_hist.history['accuracy'])
plt.plot(epochs_hist.history['val_accuracy'])

plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch number')
plt.legend(['Training Loss', 'Validation Loss'], loc= 'upper left')
plt.show()

plt.plot(epochs_hist.history['loss'])
plt.plot(epochs_hist.history['val_loss'])

plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch number')
plt.legend(['Training Loss', 'Validation Loss'], loc= 'lower left')
plt.show()

predicted = model.predict(x_test)

pred_value = []
test = []
for i in range(len(predicted)): # Assuming 'predicted' is a list or array
  pred_value.append(np.argmax(predicted[i])) # Access elements of 'predicted'

for i in y_test:
  test.append(np.argmax(i))
for i in y_test:
  test.append(np.argmax(i))

from sklearn.metrics import classification_report, confusion_matrix

# Print Confusion Matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Print Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))

"""Accuracy: Overall correct predictions.

Precision: Of all accounts predicted as fake, how many were actually fake?

Recall: Of all actual fake accounts, how many did the model catch?

F1-score: Harmonic mean of precision and recall.
"""

from sklearn.metrics import roc_curve, auc

fpr, tpr, _ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)

con_matrix = confusion_matrix(y_test, pred_value)  # Use your actual test labels and predictions

# Plotting
plt.figure(figsize=(6, 5))
sns.heatmap(con_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Genuine', 'Fake'], yticklabels=['Genuine', 'Fake'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Fake vs Genuine Accounts')
plt.tight_layout()
plt.show()

model.fit(x_train, y_train)

y_pred = model.predict(x_test)

accuracy_score(y_test, y_pred)

confusion_matrix(y_test, y_pred)

# explain model with tree plot
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
from sklearn import tree


# Create and train a DecisionTreeClassifier
dt_model = DecisionTreeClassifier()
dt_model.fit(x_train, y_train)

# Now you can use dt_model with plot_tree
plt.figure(figsize=(15, 10))
tree.plot_tree(dt_model, filled=True, feature_names=x_train.columns)
class_names = ['Genuine', 'Fake']
plt.show()

"""**What It Tells Us**:

*  Accuracy is low (53%), so the model isn't performing well on this test data.

*  Recall is poor for class 0 (genuine) â€” it's missing many genuine users.

*  The model favors predicting fake accounts (high recall, low precision).

###**Conclusion**:

In this project, we developed a machine learning/deep learning model to classify Instagram accounts into fake and genuine categories. We trained the model using a labeled dataset containing user metadata such as follower/following counts, profile picture status, and engagement statistics.

After training, the model was evaluated on a separate test dataset. The performance metrics are as follows:

*  Accuracy: 53%

*  Precision (Fake): 52%

*  Recall (Fake): 88%

*  Precision (Genuine): 63%

*  Recall (Genuine): 20%

**Future Corrections**:

The confusion matrix reveals that the model tends to favor predicting fake accounts, likely due to class imbalance or limited representation of genuine accounts in the training data. This resulted in many genuine accounts being misclassified.

To improve the model, future work may include:

*  Balancing the training dataset using techniques like SMOTE or class weighting

*  Feature engineering with more behavioral attributes

*  Exploring more complex architectures or ensemble methods

Despite the modest accuracy, the model demonstrates potential in identifying fake accounts with high recall. This makes it particularly useful for screening potentially fake users, though further refinement is needed for real-world deployment.
"""









